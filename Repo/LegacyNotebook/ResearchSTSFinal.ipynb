{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QfKc7p5PVLTi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfKc7p5PVLTi",
        "outputId": "d7c95293-a4e7-40c1-adf6-cc946b54a65c"
      },
      "outputs": [],
      "source": [
        "\"\"\"Not used in paper. Original legacy notebook\"\"\"\n",
        "pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2412a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f2412a8",
        "outputId": "466d5bcf-d8f0-4c74-a514-4596c08e34f8"
      },
      "outputs": [],
      "source": [
        "pip install -U fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46af77c-b708-4d59-827e-448d62e89677",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "d46af77c-b708-4d59-827e-448d62e89677",
        "outputId": "5db86acb-abed-461b-dcf9-73def60e6b85"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import required libraries\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import math\n",
        "from scipy.special import erfinv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from fvcore.nn import FlopCountAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12012cb9-d1f7-4f45-95d5-d10de68802b1",
      "metadata": {
        "id": "12012cb9-d1f7-4f45-95d5-d10de68802b1"
      },
      "outputs": [],
      "source": [
        "\"\"\"Set the training device to GPU\"\"\"\n",
        "if torch.backends.mps.is_available():\n",
        "  mps_device = torch.device(\"mps\")\n",
        "else:\n",
        "  mps_device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7312bef4-98dc-4e03-9f44-d57fbd6bd22b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7312bef4-98dc-4e03-9f44-d57fbd6bd22b",
        "outputId": "d9cdc468-0787-4a86-884e-c9d59df5bc16"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate the Filter Attention Matrix referenced in Figure 1 of the Paper from a given filter, with a configurable stride, padding and default value.\n",
        "\"\"\"\n",
        "\n",
        "sd = 1\n",
        "filter = torch.normal(1, 0, size = (3,3))\n",
        "img_h = 8\n",
        "img_w = 8\n",
        "filt_h = 3\n",
        "filt_w = 3\n",
        "def gen_filter_attention(img_h, img_w, stride = 1, padding = 1, filter = filter, add = 0.0):\n",
        "    filter_attention_matrix = torch.zeros((img_h*img_w,img_h*img_w))+add\n",
        "    rolls = torch.linspace(0, img_w*img_h-1, img_w*img_h, dtype = torch.int64).repeat(img_h*img_w,1)\n",
        "    offset_x = 1-padding\n",
        "    offset_y = 1-padding\n",
        "    calc_height = int((img_h+padding*2 - filter.shape[0]+1)/stride)\n",
        "    calc_width = int((img_w+padding*2 - filter.shape[1]+1)/stride)\n",
        "    if(calc_width > 0 and calc_height > 0):\n",
        "        print(calc_height)\n",
        "        print(calc_width)\n",
        "        for i in range(calc_height):\n",
        "            for j in range(calc_width):\n",
        "                if offset_y == 0:\n",
        "                    if offset_x == 0:\n",
        "                        filter_attention_matrix[j+i*calc_width][0:2] = filter[1][1:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][img_w:img_w+2] = filter[2][1:3]\n",
        "                    elif offset_x == img_w-1:\n",
        "                        filter_attention_matrix[j+i*calc_width][offset_x-1:offset_x+1] = filter[1][0:2]\n",
        "                        filter_attention_matrix[j+i*calc_width][img_w+offset_x-1:img_w+offset_x+1] = filter[2][0:2]\n",
        "                    else:\n",
        "                        filter_attention_matrix[j+i*calc_width][offset_x-1:offset_x+2] = filter[1][0:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][img_w+offset_x-1:img_w+offset_x+2] = filter[1][0:3]\n",
        "                elif offset_y == img_w-1:\n",
        "                    if offset_x == 0:\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y-1)*img_w:(offset_y-1)*img_w+2] = filter[0][1:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y)*img_w:(offset_y)*img_w+2] = filter[1][1:3]\n",
        "                    elif offset_x == img_w-1:\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y-1)*img_w+offset_x-1:(offset_y-1)*img_w+offset_x+1] = filter[0][0:2]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y)*img_w+offset_x-1:(offset_y)*img_w+offset_x+1] = filter[1][0:2]\n",
        "                    else:\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y-1)*img_w+offset_x-1:(offset_y-1)*img_w+offset_x+2] = filter[0][0:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y)*img_w+offset_x-1:(offset_y)*img_w+offset_x+2] = filter[1][0:3]\n",
        "                else:\n",
        "                    if offset_x == 0:\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y-1)*img_w:(offset_y-1)*img_w+2] = filter[0][1:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][offset_y*img_w:offset_y*img_w+2] = filter[1][1:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y+1)*img_w:(offset_y+1)*img_w+2] = filter[2][1:3]\n",
        "                    elif offset_x == img_w-1:\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y-1)*img_w+offset_x-1:(offset_y-1)*img_w+offset_x+1] = filter[0][0:2]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y)*img_w+offset_x-1:(offset_y)*img_w+offset_x+1] = filter[1][0:2]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y+1)*img_w+offset_x-1:(offset_y+1)*img_w+offset_x+1] = filter[2][0:2]\n",
        "                    else:\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y-1)*img_w+offset_x-1:(offset_y-1)*img_w+offset_x+2] = filter[0][0:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y)*img_w+offset_x-1:(offset_y)*img_w+offset_x+2] = filter[1][0:3]\n",
        "                        filter_attention_matrix[j+i*calc_width][(offset_y+1)*img_w+offset_x-1:(offset_y+1)*img_w+offset_x+2] = filter[2][0:3]\n",
        "                filter_attention_matrix[j+i*calc_width] = torch.roll(filter_attention_matrix[j+i*calc_width],-(offset_x+offset_y*img_h))\n",
        "                rolls[j+i*calc_width] = torch.roll(rolls[j+i*calc_width],(offset_x+offset_y*img_h))\n",
        "                offset_x+=stride\n",
        "            offset_x=0\n",
        "            offset_y+=stride\n",
        "    return filter_attention_matrix, rolls\n",
        "filter_attention_matrix, rolls = gen_filter_attention(img_h = 8, img_w = 8, filter=filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f318f6a-5d06-437f-96f1-cd6ec097bf06",
      "metadata": {
        "id": "3f318f6a-5d06-437f-96f1-cd6ec097bf06"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate a matrix that has stronger activatoins for nearby pixels in the image, and weaker activations for pixels farther away to induce locality bias. Distance is calcuated by L2 Norm between pixels of image. Not used in the paper.\n",
        "\"\"\"\n",
        "def gen_radial(img_h, img_w, scale = 0.5, stride = 1):\n",
        "    rolls = torch.linspace(0, img_w*img_h-1, img_w*img_h, dtype = torch.int64).repeat(img_h*img_w,1)\n",
        "    radial_attention_matrix = torch.zeros((img_h*img_w,img_h*img_w))\n",
        "    calc_w = int(img_w/stride)\n",
        "    calc_h = int(img_h/stride)\n",
        "    x_pos = 0\n",
        "    y_pos = 0\n",
        "    for curr_i in range(calc_h):\n",
        "        for curr_j in range(calc_w):\n",
        "            curr_matrix = torch.zeros((img_h, img_w))\n",
        "            for i in range(img_h):\n",
        "                for j in range(img_w):\n",
        "                    euc_weight = 1/np.exp(scale*(((y_pos-i)**2+(x_pos-j)**2)**(0.5)))\n",
        "                    curr_matrix[i][j] = euc_weight\n",
        "            radial_attention_matrix[curr_i*calc_h+curr_j] = torch.roll(curr_matrix.reshape(-1),-(y_pos*img_w+x_pos))\n",
        "            rolls[curr_i*calc_h+curr_j] = torch.roll(rolls[curr_i*calc_h+curr_j],(y_pos*img_w+x_pos))\n",
        "            x_pos+=stride\n",
        "        x_pos = 0\n",
        "        y_pos+=stride\n",
        "    return radial_attention_matrix, rolls\n",
        "radial_attention_matrix = gen_radial(img_h = 8, img_w = 8,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "niquo78mY90V",
      "metadata": {
        "id": "niquo78mY90V"
      },
      "outputs": [],
      "source": [
        "\"Generate a matrix of all ones. Induces no initial inductive bias on the Learned Mask Self Attention layer. Not used in the paper\"\n",
        "def gen_ones(img_h, img_w, stride = 1):\n",
        "    rolls = torch.linspace(0, img_w*img_h-1, img_w*img_h, dtype = torch.int64).repeat(img_h*img_w,1)\n",
        "    ones_attention_matrix = torch.normal(1,0.1,(img_h*img_w,img_h*img_w))\n",
        "    calc_w = int(img_w/stride)\n",
        "    calc_h = int(img_h/stride)\n",
        "    x_pos = 0\n",
        "    y_pos = 0\n",
        "    for curr_i in range(calc_h):\n",
        "        for curr_j in range(calc_w):\n",
        "            rolls[curr_i*calc_h+curr_j] = torch.roll(rolls[curr_i*calc_h+curr_j],(y_pos*img_w+x_pos))\n",
        "            x_pos+=stride\n",
        "        x_pos = 0\n",
        "        y_pos+=stride\n",
        "    return ones_attention_matrix, rolls\n",
        "ones_attention_matrix = gen_ones(img_h = 8, img_w = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93b96e3-8951-4de1-b18c-814942d21ac2",
      "metadata": {
        "id": "f93b96e3-8951-4de1-b18c-814942d21ac2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate a matrix that has stronger activatoins for nearby pixels in the image, and weaker activations for pixels farther away to induce locality bias. Distance is calcuated by L1 Norm between pixels of image. Not used in the paper.\n",
        "\"\"\"\n",
        "def gen_l1(img_h, img_w, scale = 0.5, stride = 1):\n",
        "    rolls = torch.linspace(0, img_w*img_h-1, img_w*img_h, dtype = torch.int64).repeat(img_h*img_w,1)\n",
        "    radial_attention_matrix = torch.zeros((img_h*img_w,img_h*img_w))\n",
        "    l1_attention_matrix = torch.zeros((img_h*img_w, img_h*img_w))\n",
        "    calc_w = int(img_w/stride)\n",
        "    calc_h = int(img_h/stride)\n",
        "    x_pos = 0\n",
        "    y_pos = 0\n",
        "    for curr_i in range(calc_h):\n",
        "        for curr_j in range(calc_w):\n",
        "            curr_matrix = torch.zeros((img_h, img_w))\n",
        "            for i in range(img_h):\n",
        "                for j in range(img_w):\n",
        "                    euc_weight = 1/np.exp(scale*(abs(y_pos-i)+abs(x_pos-j)))\n",
        "                    curr_matrix[i][j]=euc_weight\n",
        "            l1_attention_matrix[curr_i*calc_h+curr_j] = torch.roll(curr_matrix.reshape(-1),-(y_pos*img_w+x_pos))\n",
        "            rolls[curr_i*calc_h+curr_j] = torch.roll(rolls[curr_i*calc_h+curr_j],(y_pos*img_w+x_pos))\n",
        "            x_pos+=stride\n",
        "        x_pos = 0\n",
        "        y_pos+=stride\n",
        "    return l1_attention_matrix, rolls\n",
        "l1_attention_matrix = gen_l1(img_h = 8, img_w = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3fe631-865f-4d26-a2b1-646b8aa9fc80",
      "metadata": {
        "id": "8b3fe631-865f-4d26-a2b1-646b8aa9fc80"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\"\"\"Module that allows for the training two different masks, and using rank approximation to mimic the filter attention map.\"\"\"\n",
        "class MaskTrainer(nn.Module):\n",
        "    def __init__(self, seq_len = 1024, repeat_masks = True, num_masks = 16, mask_fidelity = 9, groups = 5):\n",
        "        super().__init__()\n",
        "        if(repeat_masks):\n",
        "            self.mask1 = nn.Parameter(torch.normal(0.0,0.001,(mask_fidelity, seq_len)), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(torch.normal(0.0,0.001,(mask_fidelity, seq_len)), requires_grad = True)\n",
        "        else:\n",
        "            self.mask1 = nn.Parameter(torch.normal(0.0,0.001,(groups, num_masks, mask_fidelity, seq_len)), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(torch.normal(0.0,0.001,(groups, num_masks, mask_fidelity, seq_len)), requires_grad = True)\n",
        "    def forward(self, inputs):\n",
        "        att = torch.matmul(self.mask1.transpose(-1,-2), self.mask2)\n",
        "        return att\n",
        "model = MaskTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VX5LgLU6V908",
      "metadata": {
        "id": "VX5LgLU6V908"
      },
      "outputs": [],
      "source": [
        "\"\"\"Module that allows for double rank approximation. Using two masks to approximate each mask in Rank Approximation. Those two computed masks are then used to mimic filter attention map. Not used in paper.\"\"\"\n",
        "class DoubleMaskTrainer(nn.Module):\n",
        "    def __init__(self, img_h = 32, img_w = 32, repeat_masks = True, num_masks = 16, mask_fidelity1 = 4, mask_fidelity2 = 9):\n",
        "        super().__init__()\n",
        "        self.mask_fidelity1 = mask_fidelity1\n",
        "        self.mask_fidelity2 = mask_fidelity2\n",
        "        if(repeat_masks):\n",
        "            self.mask1 = nn.Parameter(torch.normal(0.0,0.1,(mask_fidelity2,mask_fidelity1, img_h)), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(torch.normal(0.0,0.1,(mask_fidelity2,mask_fidelity1, img_w)), requires_grad = True)\n",
        "            self.mask3 = nn.Parameter(torch.normal(0.0,0.1,(mask_fidelity2,mask_fidelity1, img_h)), requires_grad = True)\n",
        "            self.mask4 = nn.Parameter(torch.normal(0.0,0.1,(mask_fidelity2,mask_fidelity1, img_w)), requires_grad = True)\n",
        "        else:\n",
        "            self.mask1 = nn.Parameter(torch.normal(0.0,0.1,(num_masks, mask_fidelity2,mask_fidelity1, img_h)), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(torch.normal(0.0,0.1,(num_masks,mask_fidelity2,mask_fidelity1, img_w)), requires_grad = True)\n",
        "            self.mask3 = nn.Parameter(torch.normal(0.0,0.1,(num_masks,mask_fidelity2,mask_fidelity1, img_h)), requires_grad = True)\n",
        "            self.mask4 = nn.Parameter(torch.normal(0.0,0.1,(num_masks,mask_fidelity2,mask_fidelity1, img_w)), requires_grad = True)\n",
        "    def forward(self, inputs):\n",
        "        att = torch.matmul(torch.matmul(self.mask1.transpose(-1,-2), self.mask2).reshape(self.mask_fidelity2,-1).transpose(-1,-2), torch.matmul(self.mask3.transpose(-1,-2), self.mask4).reshape(self.mask_fidelity2, -1))\n",
        "        return att\n",
        "model = MaskTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641adb95-2cc7-45af-af44-9603e4700f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "641adb95-2cc7-45af-af44-9603e4700f0d",
        "outputId": "1d071618-2fbf-4b55-c3b6-9059eca2cc53"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function that executes training procedure for the Learnable Masks using MaskTrainer, generating the trained masks that mimic the filter attention map, and are used for rank approximation.\n",
        "\"\"\"\n",
        "def gen_rolls_and_masks(img_sizes, patch_size = 2, filt_type = 'filter', train_type = \"single\", should_train = False, mask_fidelity = 9):\n",
        "    img_sizes = [(int(img_h/patch_size), int(img_w/patch_size)) for img_h, img_w in img_sizes]\n",
        "    rolls = []\n",
        "    masks = []\n",
        "    for j, (img_h, img_w) in enumerate(img_sizes):\n",
        "        filter = torch.normal(1, 0, size = (3,3))\n",
        "        if filt_type == 'filter':\n",
        "            print(f\"img_h = {img_h} + img_w = {img_w}\")\n",
        "            attention_matrix, roll_data = (data.to(mps_device) for data in gen_filter_attention(filter=filter, img_h = img_h, img_w = img_w, stride = 1))\n",
        "            rolls.append(roll_data)\n",
        "        elif filt_type == 'radial':\n",
        "            print(f\"img_h = {img_h} + img_w = {img_w}\")\n",
        "            attention_matrix, roll_data = (data.to(mps_device) for data in gen_radial(img_h = img_h, img_w = img_w, stride = 1))\n",
        "            rolls.append(roll_data)\n",
        "        elif filt_type == 'ones':\n",
        "            print(f\"img_h = {img_h} + img_w = {img_w}\")\n",
        "            attention_matrix, roll_data = (data.to(mps_device) for data in gen_ones(img_h = img_h, img_w = img_w, stride = 1))\n",
        "            rolls.append(roll_data)\n",
        "        else:\n",
        "            print(f\"img_h = {img_h} + img_w = {img_w}\")\n",
        "            attention_matrix, roll_data = (data.to(mps_device) for data in gen_l1(img_h = img_h, img_w = img_w, stride = 1))\n",
        "            rolls.append(roll_data)\n",
        "        plt.matshow(attention_matrix.cpu())\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        model = MaskTrainer(seq_len = img_h*img_w, mask_fidelity = mask_fidelity).to(mps_device) if train_type == \"single\" else DoubleMaskTrainer(img_h = img_h, img_w = img_w).to(mps_device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        loop_num = 10000\n",
        "        if(should_train):\n",
        "            for i in range(loop_num):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(1)\n",
        "                loss = loss_fn(outputs, attention_matrix)\n",
        "                if i % 2000 == 0:\n",
        "                    print(loss)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        if train_type == \"single\":\n",
        "            mask1 =  model.mask1\n",
        "            mask2 = model.mask2\n",
        "            masks.append((mask1, mask2))\n",
        "        else:\n",
        "            mask1 =  model.mask1\n",
        "            mask2 = model.mask2\n",
        "            mask3 = model.mask3\n",
        "            mask4 = model.mask4\n",
        "            masks.append((mask1, mask2, mask3, mask4))\n",
        "    return (rolls, masks)\n",
        "\n",
        "rolls, masks = (torch.zeros((3,3)),torch.zeros((3,3)))\n",
        "rolls, masks = gen_rolls_and_masks(img_sizes = [(32,32), (16,16), (8,8)], filt_type = 'filter',  mask_fidelity=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xsAD_4npiFtx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xsAD_4npiFtx",
        "outputId": "df2d6d28-293c-43bc-a6df-0f06419a93d7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function that executes training procedure for the Learnable Masks using MaskTrainer. All the masks are slightly different, since different architecutres might need slight differences in the mask matrix to break symetries during training. Not used in the paper.\n",
        "\"\"\"\n",
        "import copy\n",
        "img_sizes = []\n",
        "for i in range(5): img_sizes.append((32, 32))\n",
        "for i in range(3): img_sizes.append((16, 16))\n",
        "for i in range(1): img_sizes.append((8,8))\n",
        "strides = []\n",
        "for i in range(3): strides.append(1)\n",
        "for i in range(2): strides.append(2)\n",
        "for i in range(1): strides.append(1)\n",
        "for i in range(2): strides.append(2)\n",
        "for i in range(1): strides.append(1)\n",
        "block1_masks = 32\n",
        "block2_masks = 64\n",
        "block3_masks = 128\n",
        "num_masks = []\n",
        "for i in range(3): num_masks.append(block1_masks)\n",
        "for i in range(3): num_masks.append(block2_masks)\n",
        "for i in range(3): num_masks.append(block3_masks)\n",
        "def gen_rolls_and_masks_without_repeat(img_sizes = img_sizes, strides = strides, patch_size = 4, num_masks = num_masks, mask_fidelity = 9, groups = 3):\n",
        "    img_sizes = [(int(img_h/math.sqrt(patch_size)), int(img_w/math.sqrt(patch_size))) for img_h, img_w in img_sizes]\n",
        "    filters = [torch.normal(1,0.1,(groups, num_masks, 3,3)) for i, num_masks in enumerate(num_masks)]\n",
        "    rolls = []\n",
        "    masks = []\n",
        "    for j, (img_h, img_w) in enumerate(img_sizes):\n",
        "        print(f\"img_h = {img_h} + img_w = {img_w}\")\n",
        "        attention_matrix = torch.stack([torch.stack([gen_filter_attention(filter=filter, img_h = img_h, img_w = img_w, stride = strides[j])[0].to(mps_device) for filter in filter_row]) for filter_row in filters[j]])\n",
        "        print(f\"shape:{attention_matrix.shape}\")\n",
        "        roll_data = gen_filter_attention(filter=filters[j][0,0], img_h = img_h, img_w = img_w, stride = strides[j])[1].to(mps_device)\n",
        "        rolls.append(roll_data)\n",
        "        plt.matshow(attention_matrix[0,0].detach().cpu())\n",
        "        plt.matshow(roll_data.detach().cpu())\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        model = MaskTrainer(seq_len = img_h*img_w, repeat_masks=False, num_masks = num_masks[j], mask_fidelity = mask_fidelity, groups = groups).to(mps_device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        loop_num = 10000\n",
        "        for i in range(loop_num):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(1)\n",
        "            loss = loss_fn(outputs, attention_matrix)\n",
        "            if i % 2000 == 0:\n",
        "                print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        mask1 =  model.mask1\n",
        "        mask2 = model.mask2\n",
        "        masks.append((mask1, mask2))\n",
        "        seq_len = img_w*img_h\n",
        "    return (rolls, masks)\n",
        "rolls, masks = gen_rolls_and_masks_without_repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da73a09-0317-4568-bc61-878d9c87f625",
      "metadata": {
        "id": "0da73a09-0317-4568-bc61-878d9c87f625"
      },
      "source": [
        "<h2><b>CIFAR 10 Performance</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4722f2-c42f-4996-a86f-f330f66c1c5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e4722f2-c42f-4996-a86f-f330f66c1c5f",
        "outputId": "0140b2ec-02af-4cea-c48e-0ac808c924f8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Loads the CIFAR-10 Dataset, and splits it into two sections. This allows for reducing dataset size to train models on smaller dataset sizes and analyze model scalability.\"\"\"\n",
        "CIFAR10Orig = torchvision.datasets.CIFAR10(\"Datasets/CIFAR-10\",train=True, download=True)\n",
        "seed = 1\n",
        "CIFAR10, CIFAR_Val = torch.utils.data.random_split(\n",
        "        CIFAR10Orig, [40000, 10000], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "print(len(CIFAR10))\n",
        "CIFAR_Train = CIFAR10\n",
        "\n",
        "CIFAR_Test = torchvision.datasets.CIFAR10(\"Datasets/CIFAR-10\", train = False, download=True)\n",
        "print(len(CIFAR_Val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6eb43c-2dfb-49e8-9974-e25a084b06b6",
      "metadata": {
        "id": "5f6eb43c-2dfb-49e8-9974-e25a084b06b6"
      },
      "outputs": [],
      "source": [
        "\"\"\"Pytorch Dataset class that allows for easy and efficient dataloading of the CIFAR-10 dataset. Has the ability to downsample images for training without patching.\"\"\"\n",
        "class CIFAR_Conv(Dataset):\n",
        "    def __init__(self, Dataset = CIFAR_Train, train = True):\n",
        "        self.CIFAR = Dataset\n",
        "        if(train):\n",
        "          self.tensor_transform = transforms.Compose([\n",
        "              transforms.Resize((32,32)),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(0,1)\n",
        "          ])\n",
        "        else:\n",
        "          self.tensor_transform = transforms.Compose([\n",
        "              transforms.Resize((32,32)),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(0,1)\n",
        "          ])\n",
        "    def __getitem__(self, index):\n",
        "        image = self.CIFAR.__getitem__(index)\n",
        "        return (self.tensor_transform(image[0]),image[1])\n",
        "    def __len__(self):\n",
        "        return len(self.CIFAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40f9a19-468f-4f6b-8d1e-1bc04fe67ffe",
      "metadata": {
        "id": "d40f9a19-468f-4f6b-8d1e-1bc04fe67ffe"
      },
      "outputs": [],
      "source": [
        "\"\"\"Initialize all the datasets and dataloaders\"\"\"\n",
        "CIFAR_Convolve = CIFAR_Conv()\n",
        "CIFAR_Conv_Val = CIFAR_Conv(Dataset = CIFAR_Val, train = False)\n",
        "CIFAR_Convloader = DataLoader(CIFAR_Convolve, batch_size = 180, shuffle = True, pin_memory = False)\n",
        "CIFAR_Valconvloader = DataLoader(CIFAR_Conv_Val, batch_size = 180, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xknb4AMBPijn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xknb4AMBPijn",
        "outputId": "2f6d9810-719d-4c6d-80e4-366b33892db8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Configure pytorch to access unused VRAM already allocated by another program. This allows for the model to use every last bit of VRAM available on the GPU.\"\"\"\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595cb879-947a-4ce8-b6d9-813c31c790eb",
      "metadata": {
        "id": "595cb879-947a-4ce8-b6d9-813c31c790eb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Conduct patching described by paper. Spreads the pixels in each non-overlapping patch along the channel dimension.\n",
        "\"\"\"\n",
        "class Patchify(nn.Module):\n",
        "    def __init__(self, patch_size = 2):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.unfold = nn.Unfold(patch_size, stride=patch_size)\n",
        "    def forward(self, input):\n",
        "        return self.unfold(input).transpose(-1,-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d4ec3c-97ab-49bf-8237-36e3f497b3f7",
      "metadata": {
        "id": "b8d4ec3c-97ab-49bf-8237-36e3f497b3f7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Computes Batch Normalization for the ConvFormer, Vision Transformer, and the Transformer.\n",
        "\"\"\"\n",
        "class BatchNormTranspose(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.BNorm = nn.BatchNorm1d(channels)\n",
        "    def forward(self, input):\n",
        "        output = self.BNorm(input.transpose(-1,-2))\n",
        "        return output.transpose(-1,-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec59a2b4-7361-4d2c-bb93-6aba07e60c73",
      "metadata": {
        "id": "ec59a2b4-7361-4d2c-bb93-6aba07e60c73"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Appends fixed sine, and cosine positional encodings to Vision Transformer and ConvFormer.\n",
        "\"\"\"\n",
        "class PositionalEncodingAppend(nn.Module):\n",
        "    def __init__(self, total_size, device=mps_device):\n",
        "        super().__init__()\n",
        "        self.total_size = total_size\n",
        "        self.device = device\n",
        "    def forward(self, input):\n",
        "        positions = torch.arange(input.shape[-2],dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "        freq = 1/torch.exp(torch.linspace(0,8,int((self.total_size-input.shape[-1])/2), dtype=torch.float32)).view(1,-1).to(self.device)\n",
        "        sin_encodings = torch.sin(torch.matmul(positions, freq)).repeat(input.shape[0],1,1)\n",
        "        cos_encodings = torch.cos(torch.matmul(positions, freq)).repeat(input.shape[0],1,1)\n",
        "        return torch.cat((input, sin_encodings, cos_encodings), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0nnwpJALapNQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nnwpJALapNQ",
        "outputId": "39ff2985-62a2-4fb6-be2f-c1f5c2bb8490"
      },
      "outputs": [],
      "source": [
        "\"\"\"This is the implementation of the Learned Masked Self Attention Layer described in the paper. It is implemented according to the algorithm detailed in the paper.\"\"\"\n",
        "import math\n",
        "import torch.autograd.profiler as profiler\n",
        "import gc\n",
        "class MultiScaledConvHead(nn.Module):\n",
        "    def __init__(self, num_heads, d_model, mask1, mask2, num_masks, roll_matrix, reduction = False, patch_size = 2, repeat_masks = True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_masks = num_masks\n",
        "        self.Linears = nn.ModuleList(nn.Linear(d_model, d_model) for __ in range(3))\n",
        "        if(repeat_masks):\n",
        "            self.mask1 = nn.Parameter(mask1.clone().repeat(num_masks, 1, 1), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(mask2.clone().repeat(num_masks, 1, 1), requires_grad = True)\n",
        "        else:\n",
        "            self.mask1 = nn.Parameter(mask1.clone(), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(mask2.clone(), requires_grad = True)\n",
        "        self.register_buffer(name='roll_back', tensor=roll_matrix.clone())\n",
        "        self.roll_back = self.roll_back\n",
        "        self.reduction = reduction\n",
        "        self.patch_size = patch_size\n",
        "        self.patchify = Patchify(patch_size=2)\n",
        "        self.epsilon = 0.00000001\n",
        "        self.final_linear = nn.Linear(d_model, d_model)\n",
        "    def forward(self, input):\n",
        "        keys, queries, values = (\n",
        "            linear(inp).reshape(input.shape[0], -1, self.num_heads, int(self.d_model/self.num_heads)).transpose(1,2)\n",
        "            for linear, inp in zip(self.Linears, (input, input, input))\n",
        "        )\n",
        "        learnable_mask = torch.matmul(self.mask1.transpose(-1,-2), self.mask2)\n",
        "        # print(learnable_mask.shape)\n",
        "        att = torch.matmul(queries,keys.transpose(-1,-2))/math.sqrt(self.d_model/self.num_heads)\n",
        "        att = att.reshape(input.shape[0], int(self.num_heads/self.num_masks),self.num_masks, att.shape[-2],att.shape[-1])\n",
        "        del keys\n",
        "        del queries\n",
        "        att = att*torch.gather(learnable_mask,-1,self.roll_back.expand_as(learnable_mask)).expand_as(att)\n",
        "        # print(att.shape)\n",
        "        del learnable_mask\n",
        "        att = att.reshape(input.shape[0], self.num_heads, att.shape[-2], att.shape[-1])\n",
        "        #Final attention normed to prevent exploding and vanishing gradients.\n",
        "        att = att/(torch.norm(att, dim = (-1,-2))[..., None,None]+self.epsilon)\n",
        "        outputs = torch.matmul(att,values).transpose(1,2).reshape(input.shape[0], -1, self.d_model)\n",
        "        outputs = self.final_linear(outputs)\n",
        "        if self.reduction:\n",
        "            outputs = outputs.reshape(outputs.shape[0], int(math.sqrt(outputs.shape[-2])),int(math.sqrt(outputs.shape[-2])), outputs.shape[-1])\n",
        "            outputs = outputs.permute(0,3,1,2)\n",
        "            outputs = self.patchify(outputs)\n",
        "        del att\n",
        "        del values\n",
        "        return outputs\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da5XB5hxmyvv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "da5XB5hxmyvv",
        "outputId": "af8d6f40-40e3-4ef2-a19f-2fbe2c69eb7d"
      },
      "outputs": [],
      "source": [
        "\"\"\"Test LMSA Layer\"\"\"\n",
        "head = MultiScaledConvHead(num_heads = 16, d_model = 32, reduction = False, mask1=masks[0][0], mask2=masks[0][0], num_masks = 16, roll_matrix=rolls[0]).to(mps_device)\n",
        "input = torch.zeros(10, 256, 32).to(mps_device)\n",
        "print(input)\n",
        "output = head(input)\n",
        "print(output.shape)\n",
        "del head \n",
        "del input\n",
        "del output\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OHcqKZbZ5wOO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHcqKZbZ5wOO",
        "outputId": "59bb3186-ad23-403f-a70b-6f791f504990"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.autograd.profiler as profiler\n",
        "import gc\n",
        "\"\"\"Allow more masks per head in LMSA to copmletely represent convolution. Not covered in paper.\"\"\"\n",
        "class MultiScaledConvFilterHead(nn.Module):\n",
        "    def __init__(self, scale = 2, num_heads = 16, d_model = 48, seq_len = 256, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0], reduction = 64, groups = 1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.scale = scale\n",
        "        self.Linears = nn.ModuleList(nn.Linear(d_model, scale*d_model) for __ in range(3))\n",
        "        self.mask1 = nn.Parameter(mask1.clone(), requires_grad = True)\n",
        "        self.mask2 = nn.Parameter(mask2.clone(), requires_grad = True)\n",
        "        self.register_buffer(name='roll_back', tensor=roll_matrix.clone())\n",
        "        self.roll_back = self.roll_back[...,:reduction,:]\n",
        "        self.reduction = reduction\n",
        "        self.groups = groups\n",
        "        self.epsilon = 0.00000001\n",
        "    def forward(self, input):\n",
        "        with profiler.record_function(\"KEY_QUERY_VALUE_PROJECTION\"):\n",
        "            keys, queries, values = (\n",
        "                linear(inp).reshape(input.shape[0], -1, self.num_heads, int(self.d_model*self.scale/self.num_heads)).transpose(1,2)\n",
        "                for linear, inp in zip(self.Linears, (input, input, input))\n",
        "            )\n",
        "\n",
        "        with profiler.record_function(\"LEARNABLE_MASK_GEN\"):\n",
        "            learnable_mask = torch.matmul(self.mask1.transpose(-1,-2), self.mask2)[...,:self.reduction,:]\n",
        "        with profiler.record_function(\"ATTENTION_MATMUL\"):\n",
        "            att = torch.matmul(queries,keys.transpose(-1,-2))[...,:self.reduction,:]/math.sqrt(self.d_model*self.scale/self.num_heads)\n",
        "\n",
        "        with profiler.record_function(\"APPLY_LEARNABLE_MASK\"):\n",
        "\n",
        "            att = att.reshape(input.shape[0], 1, 1, self.num_heads, att.shape[-2],att.shape[-1]).expand(-1,self.groups,int(self.d_model*self.scale/self.num_heads), -1,-1,-1)\n",
        "\n",
        "            learnable_mask = learnable_mask.reshape(self.groups, self.num_heads, -1, learnable_mask.shape[-2], learnable_mask.shape[-1]).transpose(1,2)\n",
        "            del keys\n",
        "            del queries\n",
        "            att = att*torch.gather(learnable_mask,-1,self.roll_back.expand_as(learnable_mask)).expand_as(att)\n",
        "            att = torch.sum(att.transpose(2,3).reshape(input.shape[0], self.groups, self.d_model*self.scale, att.shape[-2], att.shape[-1]),dim=1)\n",
        "\n",
        "            del learnable_mask\n",
        "\n",
        "\n",
        "            att = att/(torch.norm(att, dim = (-1,-2))[..., None,None]+self.epsilon)\n",
        "\n",
        "        values = values.transpose(1,2).reshape(input.shape[0], -1, self.d_model*self.scale).transpose(-1,-2).reshape(input.shape[0], self.d_model*self.scale, -1, 1)\n",
        "        outputs = torch.matmul(att, values)\n",
        "        outputs = outputs\n",
        "        outputs = outputs.reshape(input.shape[0], self.d_model*self.scale, -1).transpose(-1,-2)\n",
        "        del att\n",
        "        del values\n",
        "\n",
        "        return outputs\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m4bRuzuG8tnc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "m4bRuzuG8tnc",
        "outputId": "893bf2aa-7498-45d6-98ec-dfec862a0260"
      },
      "outputs": [],
      "source": [
        "\"\"\"Test Multi-Filter LMSA Layer\"\"\"\n",
        "head = MultiScaledConvFilterHead().to(mps_device)\n",
        "input = torch.zeros(1, 256, 48).to(mps_device)\n",
        "for i in range(48):\n",
        "    input[:,:,i] = torch.full((1,256),i).to(mps_device)\n",
        "print(input)\n",
        "outputs = head(input)\n",
        "print(outputs.shape)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a645629-bc81-4990-81b1-be4e772d8e4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "8a645629-bc81-4990-81b1-be4e772d8e4d",
        "outputId": "de1bd8a3-c4ba-44a8-ad94-e738fb183404"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LMSA Layer with double rank approximation, using DoubleMaskTrainer to retrieve weights. Not used in Paper.\n",
        "\"\"\"\n",
        "class DoubleMultiScaledConvHead(nn.Module):\n",
        "    def __init__(self, scale = 45, num_heads = 9, d_model = 1, seq_len = 784, mask1 = masks[0][0], mask2 = masks[0][1], mask3 = masks[0][2], mask4 = masks[0][3], num_masks = 1, roll_matrix = rolls[0], reduction = 784, repeat_masks = False):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.scale = scale\n",
        "        self.num_masks = num_masks\n",
        "        self.Linears = nn.ModuleList(nn.Linear(d_model, scale*d_model) for __ in range(3))\n",
        "        if(repeat_masks):\n",
        "            self.mask1 = nn.Parameter(mask1.clone().repeat(num_masks, 1, 1), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(mask2.clone().repeat(num_masks, 1, 1), requires_grad = True)\n",
        "            self.mask3 = nn.Parameter(mask3.clone().repeat(num_masks, 1, 1), requires_grad = True)\n",
        "            self.mask4 = nn.Parameter(mask4.clone().repeat(num_masks, 1, 1), requires_grad = True)\n",
        "        else:\n",
        "            self.mask1 = nn.Parameter(mask1.clone(), requires_grad = True)\n",
        "            self.mask2 = nn.Parameter(mask2.clone(), requires_grad = True)\n",
        "            self.mask3 = nn.Parameter(mask1.clone(), requires_grad = True)\n",
        "            self.mask4 = nn.Parameter(mask2.clone(), requires_grad = True)\n",
        "        self.register_buffer(name='roll_back', tensor=roll_matrix.clone())\n",
        "        self.roll_back = self.roll_back[...,:reduction,:]\n",
        "        self.mult1 = nn.Parameter(torch.full((1,1),-2.197), requires_grad = True)\n",
        "        self.mult2 = nn.Parameter(torch.full((1,1),2.197), requires_grad = True)\n",
        "        self.reduction = reduction\n",
        "    def forward(self, input):\n",
        "        with profiler.record_function(\"KEY_QUERY_VALUE_PROJECTION\"):\n",
        "            keys, queries, values = (\n",
        "                linear(inp).reshape(input.shape[0], self.num_heads, -1, int(self.d_model*self.scale/self.num_heads))\n",
        "                for linear, inp in zip(self.Linears, (input, input, input))\n",
        "            )\n",
        "        with profiler.record_function(\"LEARNABLE_MASK_GEN\"):\n",
        "            learnable_mask = torch.matmul(torch.matmul(self.mask1.transpose(-1,-2), self.mask2).reshape(self.mask_fidelity2,-1).transpose(-1,-2), torch.matmul(self.mask3.transpose(-1,-2), self.mask4).reshape(self.mask_fidelity2, -1))[...,:self.reduction,:]\n",
        "        with profiler.record_function(\"ATTENTION_MATMUL\"):\n",
        "            att = torch.matmul(queries,keys.transpose(-1,-2))[...,:self.reduction,:]/math.sqrt(self.d_model*self.scale/self.num_heads)\n",
        "\n",
        "        with profiler.record_function(\"APPLY_LEARNABLE_MASK\"):\n",
        "            att = att.reshape(input.shape[0], int(self.num_heads/self.num_masks), self.num_masks, att.shape[-2],att.shape[-1])\n",
        "            del keys\n",
        "            del queries\n",
        "            att = nn.functional.sigmoid(self.mult1)*att+nn.functional.sigmoid(self.mult2)*torch.gather(learnable_mask,2,self.roll_back.expand_as(learnable_mask)).expand_as(att)\n",
        "            del learnable_mask\n",
        "            att = att.reshape(input.shape[0], -1, att.shape[-2], att.shape[-1])\n",
        "        outputs = torch.matmul(att,values).view(input.shape[0], self.reduction, -1)\n",
        "        del att\n",
        "        del values\n",
        "        return outputs\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7953ca2-6475-4ebb-8985-85b9570c7f34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7953ca2-6475-4ebb-8985-85b9570c7f34",
        "outputId": "ca4e76bd-e1c9-4b6f-9abc-4baf84b8b4ab"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Multi-Head Attention Layer detailed in Attention is All You Need (Vaswani, 2017) paper. Implemented to compare Transformers with the same ConvFormer architecture and Vision Transformers to ConvFormers and ResNets.\n",
        "\"\"\"\n",
        "import math\n",
        "import torch.autograd.profiler as profiler\n",
        "import gc\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.Linears = nn.ModuleList(nn.Linear(d_model, d_model) for __ in range(3))\n",
        "        self.final_linear = nn.Linear(d_model, d_model)\n",
        "    def forward(self, input):\n",
        "        with profiler.record_function(\"KEY_QUERY_VALUE_PROJECTION\"):\n",
        "            input = input.reshape(input.shape[0], self.d_model, -1).transpose(-1,-2)  # Transpose to (batch_size, seq_len, d_model)\n",
        "            keys, queries, values = (\n",
        "                linear(inp).reshape(input.shape[0], -1, self.num_heads, int(self.d_model/self.num_heads)).transpose(1,2)\n",
        "                for linear, inp in zip(self.Linears, (input, input, input))\n",
        "            )\n",
        "        with profiler.record_function(\"ATTENTION_MATMUL\"):\n",
        "            att = torch.matmul(queries,keys.transpose(-1,-2))/math.sqrt(self.d_model/self.num_heads)\n",
        "            att = torch.nn.functional.softmax(att, dim=-1)\n",
        "        outputs = torch.matmul(att,values).transpose(1,2).reshape(input.shape[0], -1, self.d_model)\n",
        "        outputs = self.final_linear(outputs)\n",
        "        return outputs\n",
        "\n",
        "model = MultiHeadAttention(num_heads=16, d_model=32)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6mBkdYw7p98j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mBkdYw7p98j",
        "outputId": "db987d50-c035-4474-98ce-8e4e2609dfb1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Multi-Head Attention Layer detailed in Attention is All You Need (Vaswani, 2017) paper. Implemented to compare Transformers with the same ConvFormer architecture and Vision Transformers to ConvFormers and ResNets.\n",
        "\"\"\"\n",
        "import math\n",
        "import torch.autograd.profiler as profiler\n",
        "import gc\n",
        "class MultiHeadAttentionWReduction(nn.Module):\n",
        "    def __init__(self, num_heads, d_model, patch_size = 1, reduction = False):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.patch_size = patch_size\n",
        "        self.Linears = nn.ModuleList(nn.Linear(d_model, d_model) for __ in range(3))\n",
        "        self.final_linear = nn.Linear(self.d_model, self.d_model)\n",
        "        self.reduction = reduction\n",
        "        self.patchify = Patchify(patch_size=patch_size)\n",
        "    def forward(self, input):\n",
        "        with profiler.record_function(\"KEY_QUERY_VALUE_PROJECTION\"):\n",
        "            keys, queries, values = (\n",
        "                linear(inp).reshape(input.shape[0], -1, self.num_heads, int(self.d_model/self.num_heads)).transpose(1,2)\n",
        "                for linear, inp in zip(self.Linears, (input, input, input))\n",
        "            )\n",
        "        with profiler.record_function(\"ATTENTION_MATMUL\"):\n",
        "            att = torch.matmul(queries,keys.transpose(-1,-2))/math.sqrt(self.d_model/self.num_heads)\n",
        "            att = torch.nn.functional.softmax(att, dim=-1)\n",
        "        outputs = torch.matmul(att,values).transpose(1,2).reshape(input.shape[0], -1, self.d_model)\n",
        "        outputs = self.final_linear(outputs)\n",
        "        if self.reduction:\n",
        "            outputs = outputs.reshape(outputs.shape[0], int(math.sqrt(outputs.shape[-2])),int(math.sqrt(outputs.shape[-2])), outputs.shape[-1])\n",
        "            outputs = outputs.permute(0,3,1,2)\n",
        "            outputs = self.patchify(outputs)\n",
        "        return outputs\n",
        "\n",
        "model = MultiHeadAttention(num_heads=16, d_model=32)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2K58S0jEqE8_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2K58S0jEqE8_",
        "outputId": "2619440c-e5cb-43a6-a218-a93eba9bcf6d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Test Multi-Head Attention Layer\n",
        "\"\"\"\n",
        "head = MultiHeadAttentionWReduction(num_heads=16, d_model=32, reduction=True, patch_size=2).to(mps_device)\n",
        "input = torch.zeros(1, 256, 32).to(mps_device)\n",
        "print(input)\n",
        "outputs = head(input)\n",
        "print(outputs.shape)\n",
        "print(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06d5f38-f7bc-45e0-be02-745666951306",
      "metadata": {
        "id": "a06d5f38-f7bc-45e0-be02-745666951306"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementation of one VisionTransformer block as described in the Vision Transformer paper (Dosovitsky, 2021).\"\"\"\n",
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, scale = 1, num_heads = 4, d_model = 1):\n",
        "        super().__init__()\n",
        "        self.BNorm1 = nn.LayerNorm(d_model)\n",
        "        self.attention = MultiHeadAttention(num_heads = num_heads, d_model = d_model)\n",
        "        self.BNorm2 = nn.LayerNorm(d_model)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(d_model, 4*d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*d_model, d_model)\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        out0 = self.BNorm1(input)\n",
        "        out1 = self.attention(out0)+out0\n",
        "        out2 = self.BNorm2(out1)\n",
        "        out3 = self.MLP(out2)+out2\n",
        "        return out3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e37e74a-3032-4f45-8504-811418da5952",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e37e74a-3032-4f45-8504-811418da5952",
        "outputId": "603e82ff-1557-4c01-e1b6-4bdd972596f0"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implements full Vision Transformer described in Vision Transformer paper.\"\"\"\n",
        "img_h = 32\n",
        "img_w = 32\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_h, img_w):\n",
        "        super().__init__()\n",
        "        patch_size = 2\n",
        "        d_model1 = 96\n",
        "        self.conv_scale_up = nn.Sequential(\n",
        "            nn.Conv2d(3,8,3,padding=1),\n",
        "            Patchify(patch_size = patch_size),\n",
        "            PositionalEncodingAppend(d_model1, device = mps_device)\n",
        "        )\n",
        "        block_list = [ViTBlock(num_heads = 16, d_model = d_model1) for i in range(8)]\n",
        "        self.ViTBlocks = nn.Sequential(\n",
        "            *block_list\n",
        "        )\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(d_model1*int(img_h*img_w/(patch_size**2)), 10),\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        out0 = self.conv_scale_up(input)\n",
        "        out1 = self.ViTBlocks(out0)\n",
        "        out1 = out1.reshape(out0.shape[0], -1)\n",
        "        out2 = self.fcn(out1)\n",
        "        return out1\n",
        "model = VisionTransformer(img_h = 32, img_w = 32).to(mps_device)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403d2e16-136a-45a9-9186-68d4b98c82fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "403d2e16-136a-45a9-9186-68d4b98c82fd",
        "outputId": "f7d8a039-39cb-4ab5-9e8d-78c5b0daacb2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Test Vision Transformer\"\"\"\n",
        "model = VisionTransformer(img_h = 32, img_w = 32).to(mps_device)\n",
        "conv_input = torch.stack([CIFAR_Convolve.__getitem__(i)[0] for i in range(16)]).to(mps_device)\n",
        "print(conv_input.shape)\n",
        "conv_output = model(conv_input)\n",
        "%timeit model(conv_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OVuUouzWSK2S",
      "metadata": {
        "id": "OVuUouzWSK2S"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implements Transformer with the same model architecture as ResNets and ConvFormers to show that architectural differences are not responsible for the difference in performance between Transformers and ConvFormers.\"\"\"\n",
        "patch_size = 4\n",
        "class TransformerResNet(nn.Module):\n",
        "    def __init__(self, img_h, img_w):\n",
        "        super().__init__()\n",
        "        res_block1_heads = 16\n",
        "        res_block2_heads = 32\n",
        "        res_block3_heads = 64\n",
        "        #Should be square number\n",
        "        patch_size = 2\n",
        "        d_model1 = 32\n",
        "        d_model2 = d_model1*(patch_size**2)\n",
        "        d_model3 = d_model2*(patch_size**2)\n",
        "        dropout_p = 0.01\n",
        "        self.conv_scale_up = nn.Sequential(\n",
        "            nn.Conv2d(3,4,3,padding=1),\n",
        "            Patchify(patch_size = patch_size),\n",
        "            PositionalEncodingAppend(d_model1, device = mps_device)\n",
        "        )\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_13 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_14 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection1 = MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = True, patch_size = 2)\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = True, patch_size = 2),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = False),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_22 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = False),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = False),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_23 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = False),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = False),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection2 = MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = True, patch_size = 2)\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = True, patch_size = 2),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block3_heads, d_model = d_model3, reduction = False),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_32 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block3_heads, d_model = d_model3, reduction = False),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block3_heads, d_model = d_model3, reduction = False),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_33 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block3_heads, d_model = d_model3, reduction = False),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block3_heads, d_model = d_model3, reduction = False),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(d_model3, 10)\n",
        "        )\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, input):\n",
        "        out0 = self.conv_scale_up(input)\n",
        "        out1 = self.res_block_11(out0)+out0\n",
        "        out2 = self.res_block_12(out1)+out1\n",
        "        out3 = self.res_block_13(out2)+out2\n",
        "        out4 = self.res_block_14(out3)+out3\n",
        "        out5 = self.res_block_21(out4)+self.shortcut_projection1(out4)\n",
        "        out6 = self.res_block_22(out5)+out5\n",
        "        out7 = self.res_block_23(out6)+out6\n",
        "        out8 = self.res_block_31(out7)+self.shortcut_projection2(out7)\n",
        "        out9 = self.res_block_32(out8)+out8\n",
        "        out10 = self.res_block_33(out9)+out9\n",
        "        out10 = torch.mean(out10, dim=-2)\n",
        "        out11 = self.fcn(out10)\n",
        "        out11 = out11 - torch.amax(out11, 1, keepdim=True)\n",
        "        out11 = self.LogSoftmax(out11)\n",
        "        return out11\n",
        "model = TransformerResNet(img_h=32, img_w=32).to(mps_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bxmi7cDFYJEE",
      "metadata": {
        "id": "Bxmi7cDFYJEE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implements Reduced-Size Transformer with the same model architecture as ResNets and ConvFormers.\n",
        "\"\"\"\n",
        "patch_size = 2\n",
        "class TransformerResNetSmall(nn.Module):\n",
        "    def __init__(self, img_h, img_w, patch_size = 2):\n",
        "        super().__init__()\n",
        "        res_block1_heads = 16\n",
        "        res_block2_heads = 32\n",
        "        res_block3_heads = 64\n",
        "        #Should be square number\n",
        "        patch_size = 2\n",
        "        d_model1 = 32\n",
        "        d_model2 = d_model1*(patch_size**2)\n",
        "        d_model3 = d_model2*(patch_size**2)\n",
        "        dropout_p = 0.01\n",
        "        self.conv_scale_up = nn.Sequential(\n",
        "            nn.Conv2d(3,4,3,padding=1),\n",
        "            Patchify(patch_size = patch_size),\n",
        "            PositionalEncodingAppend(d_model1, device = mps_device)\n",
        "        )\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = False),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection1 = MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = True, patch_size=patch_size)\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block1_heads, d_model = d_model1, reduction = True, patch_size=patch_size),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = False),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection2 = MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = True, patch_size=patch_size)\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block2_heads, d_model = d_model2, reduction = True, patch_size=patch_size),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiHeadAttentionWReduction(num_heads = res_block3_heads, d_model = d_model3, reduction = False),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(d_model3, 10)\n",
        "        )\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, input):\n",
        "        out0 = self.conv_scale_up(input)\n",
        "\n",
        "        out1 = self.res_block_11(out0)+out0\n",
        "\n",
        "        out2 = self.res_block_12(out1)+out1\n",
        "\n",
        "        out3 = self.res_block_21(out2)+self.shortcut_projection1(out2)\n",
        "\n",
        "        out4 = self.res_block_31(out3)+self.shortcut_projection2(out3)\n",
        "\n",
        "        out5 = torch.mean(out4, dim=-2)\n",
        "\n",
        "        out6 = self.fcn(out5)\n",
        "\n",
        "        out6 = out6 - torch.amax(out6, 1, keepdim=True)\n",
        "        out6 = self.LogSoftmax(out6)\n",
        "        return out6\n",
        "model = TransformerResNetSmall(img_h=32, img_w=32).to(mps_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281189b8-ff7f-4e97-86a4-0e403b85e195",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "281189b8-ff7f-4e97-86a4-0e403b85e195",
        "outputId": "f7385bce-5fb9-4a3d-8735-01f48fbd0503"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementation for ConvFormers, the main model presented in the paper.\"\"\"\n",
        "patch_size = 2\n",
        "rolls, masks = gen_rolls_and_masks(img_sizes=[(32,32),(16,16), (8,8)], patch_size = patch_size, filt_type = \"filter\", mask_fidelity = 3, should_train=True)\n",
        "class ConvFormerResNet(nn.Module):\n",
        "    def __init__(self, img_h, img_w, patch_size, masks = masks, rolls = rolls):\n",
        "        super().__init__()\n",
        "        res_block1_heads = 16\n",
        "        res_block2_heads = 32\n",
        "        res_block3_heads = 64\n",
        "        res_block1_masks = 16\n",
        "        res_block2_masks = 32\n",
        "        res_block3_masks = 64\n",
        "        #Should be square number\n",
        "        self.patch_size = patch_size\n",
        "        self.reduce_patch_size = 2\n",
        "        d_model1 = 32\n",
        "        d_model2 = d_model1*(self.reduce_patch_size**2)\n",
        "        d_model3 = d_model2*(self.reduce_patch_size**2)\n",
        "        dropout_p = 0.01\n",
        "        self.img_h = img_h\n",
        "        self.img_w = img_w\n",
        "        self.conv_scale_up = nn.Sequential(\n",
        "            nn.Conv2d(3,4,3,padding=1),\n",
        "            Patchify(patch_size = self.patch_size),\n",
        "            PositionalEncodingAppend(d_model1, device = mps_device)\n",
        "        )\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_13 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_14 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection1 = MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0])\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = False, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_22 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = False, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = False, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_23 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = False, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = False, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection2 = MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1])\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block3_heads, d_model = d_model3, num_masks = res_block3_masks, reduction = False, mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_32 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block3_heads, d_model = d_model3, num_masks = res_block3_masks, reduction = False, mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block3_heads, d_model = d_model3, num_masks = res_block3_masks, reduction = False, mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_33 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block3_heads, d_model = d_model3, num_masks = res_block3_masks, reduction = False, mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block3_heads, d_model = d_model3, num_masks = res_block3_masks, reduction = False, mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(d_model3, 10)\n",
        "        )\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, input):\n",
        "        out0 = self.conv_scale_up(input)\n",
        "        out1 = self.res_block_11(out0)+out0\n",
        "        out2 = self.res_block_12(out1)+out1\n",
        "        out3 = self.res_block_13(out2)+out2\n",
        "        out4 = self.res_block_14(out3)+out3\n",
        "\n",
        "        out5 = self.res_block_21(out4)+self.shortcut_projection1(out4)\n",
        "        out6 = self.res_block_22(out5)+out5\n",
        "        out7 = self.res_block_23(out6)+out6\n",
        "        out8 = self.res_block_31(out7)+self.shortcut_projection2(out7)\n",
        "        out9 = self.res_block_32(out8)+out8\n",
        "        out10 = self.res_block_33(out9)+out9\n",
        "        out10 = torch.mean(out10, dim=-2)\n",
        "        out11 = self.fcn(out10)\n",
        "        out11 = out11 - torch.amax(out11, 1, keepdim=True)\n",
        "        out11 = self.LogSoftmax(out11)\n",
        "        return out11\n",
        "# model = ConvFormerResNet(img_h = 32, img_w = 32, patch_size=2).to(mps_device)\n",
        "# conv_input = torch.stack([CIFAR_Conv_Val.__getitem__(random.randint(0, 9999))[0] for i in range(1)]).to(mps_device)\n",
        "# print(model(conv_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jocUPueqTXtk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jocUPueqTXtk",
        "outputId": "ceabcacf-eca9-4cda-e44d-8d505b45fae2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of Reduced-Size ConvFormers allowing for comparisons with ResNets which have similar amounts of parameters.\n",
        "\"\"\"\n",
        "patch_size = 2\n",
        "rolls, masks = gen_rolls_and_masks(img_sizes=[(32,32),(16,16), (8,8)], patch_size = patch_size, filt_type = \"filter\", mask_fidelity = 3, should_train = True)\n",
        "class ConvFormerResNetSmall(nn.Module):\n",
        "    def __init__(self, img_h, img_w, patch_size, masks = masks, rolls = rolls):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.reduce_patch_size = 2\n",
        "        res_block1_heads = 16\n",
        "        res_block2_heads = res_block1_heads*2\n",
        "        res_block3_heads = res_block2_heads*2\n",
        "        res_block1_masks = 16\n",
        "        res_block2_masks = res_block1_masks*2\n",
        "        res_block3_masks = res_block2_masks*2\n",
        "        #Should be square number\n",
        "        patch_size = 2\n",
        "        d_model1 = 32\n",
        "        d_model2 = d_model1*(self.reduce_patch_size**2)\n",
        "        d_model3 = d_model2*(self.reduce_patch_size**2)\n",
        "        dropout_p = 0.02\n",
        "        self.conv_scale_up = nn.Sequential(\n",
        "            nn.Conv2d(3,4,3,padding=1),\n",
        "            Patchify(patch_size = self.patch_size),\n",
        "            PositionalEncodingAppend(d_model1, device = mps_device),\n",
        "        )\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = False, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection1 = MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0])\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block1_heads, d_model = d_model1, num_masks = res_block1_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = False, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection2 = MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1])\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvHead(num_heads = res_block2_heads, d_model = d_model2, num_masks = res_block2_masks, reduction = True, patch_size = self.reduce_patch_size, mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvHead(num_heads = res_block3_heads, d_model = d_model3, num_masks = res_block3_masks, reduction = False, mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(d_model3, 10)\n",
        "        )\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, input):\n",
        "        out0 = self.conv_scale_up(input)\n",
        "        print(out0.shape)\n",
        "        out1 = self.res_block_11(out0)+out0\n",
        "\n",
        "        out2 = self.res_block_12(out1)+out1\n",
        "\n",
        "        out3 = self.res_block_21(out2)+self.shortcut_projection1(out2)\n",
        "\n",
        "        out4 = self.res_block_31(out3)+self.shortcut_projection2(out3)\n",
        "\n",
        "        out5 = torch.mean(out4, dim=-2)\n",
        "\n",
        "        out6 = self.fcn(out5)\n",
        "\n",
        "        out6 = out6 - torch.amax(out6, 1, keepdim=True)\n",
        "        out6 = self.LogSoftmax(out6)\n",
        "        return out6\n",
        "# model = ConvFormerResNetSmall(img_h = 32, img_w = 32, patch_size=1).to(mps_device)\n",
        "# conv_input = torch.stack([CIFAR_Conv_Val.__getitem__(random.randint(0, 9999))[0] for i in range(1)]).to(mps_device)\n",
        "# print(model(conv_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7niRfGMOENt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "f7niRfGMOENt",
        "outputId": "4dcc56bd-a8ce-49b3-8060-e06bdf198734"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of ConvFormers that can completely represent convolution. Not covered in paper.\n",
        "\"\"\"\n",
        "patch_size = 64\n",
        "rolls, masks = gen_rolls_and_masks_without_repeat(patch_size = patch_size, mask_fidelity = 3, groups = 1)\n",
        "class ConvFormerResNetFilterSmall(nn.Module):\n",
        "    def __init__(self, img_h = img_h, img_w = img_w, masks = masks, rolls = rolls):\n",
        "        super().__init__()\n",
        "        res_block1_heads = 8\n",
        "        res_block2_heads = 32\n",
        "        res_block3_heads = 64\n",
        "        #Should be square number\n",
        "        patch_size = 64\n",
        "        d_model1 = 8\n",
        "        d_model2 = d_model1*2\n",
        "        d_model3 = d_model2*2\n",
        "        dropout_p = 0.02\n",
        "        self.conv_scale_up = nn.Sequential(\n",
        "            nn.Conv2d(3,4,3,padding=1),\n",
        "            Patchify(patch_size = int(math.sqrt(patch_size))),\n",
        "            PositionalEncodingAppend(d_model1, device = mps_device)\n",
        "        )\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            MultiScaledConvFilterHead(scale = 1, num_heads = res_block1_heads, d_model = d_model1, seq_len = int(1024/patch_size), reduction = int(1024/patch_size), mask1 = masks[0][0], mask2 = masks[0][1], roll_matrix = rolls[0]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvFilterHead(scale = 1, num_heads = res_block1_heads, d_model = d_model1, seq_len = int(1024/patch_size), reduction = int(1024/patch_size), mask1 = masks[1][0], mask2 = masks[1][1], roll_matrix = rolls[1]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvFilterHead(scale = 1, num_heads = res_block1_heads, d_model = d_model1, seq_len = int(1024/patch_size), reduction = int(1024/patch_size), mask1 = masks[2][0], mask2 = masks[2][1], roll_matrix = rolls[2]),\n",
        "            BatchNormTranspose(d_model1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection1 = MultiScaledConvFilterHead(scale = 2, num_heads = res_block1_heads, d_model = d_model1, seq_len = int(1024/patch_size), reduction = int(256/patch_size), mask1 = masks[3][0], mask2 = masks[3][1], roll_matrix = rolls[3])\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvFilterHead(scale = 2, num_heads = res_block1_heads, d_model = d_model1, seq_len = int(1024/patch_size), reduction = int(256/patch_size), mask1 = masks[4][0], mask2 = masks[4][1], roll_matrix = rolls[4]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvFilterHead(scale = 1, num_heads = res_block2_heads, d_model = d_model2, seq_len = int(256/patch_size), reduction = int(256/patch_size), mask1 = masks[5][0], mask2 = masks[5][1], roll_matrix = rolls[5]),\n",
        "            BatchNormTranspose(d_model2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.shortcut_projection2 = MultiScaledConvFilterHead(scale = 2, num_heads = res_block2_heads, d_model = d_model2, seq_len = int(256/patch_size), reduction = int(64/patch_size), mask1 = masks[6][0], mask2 = masks[6][1], roll_matrix = rolls[6])\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MultiScaledConvFilterHead(scale = 2, num_heads = res_block2_heads, d_model = d_model2, seq_len = int(256/patch_size), reduction = int(64/patch_size), mask1 = masks[7][0], mask2 = masks[7][1], roll_matrix = rolls[7]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            MultiScaledConvFilterHead(scale = 1, num_heads = res_block3_heads, d_model = d_model3, seq_len = int(64/patch_size), reduction = int(64/patch_size), mask1 = masks[8][0], mask2 = masks[8][1], roll_matrix = rolls[8]),\n",
        "            BatchNormTranspose(d_model3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(d_model3, 10)\n",
        "        )\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, input):\n",
        "        out0 = self.conv_scale_up(input)\n",
        "\n",
        "        out1 = self.res_block_11(out0)+out0\n",
        "\n",
        "        out2 = self.res_block_12(out1)+out1\n",
        "\n",
        "        out3 = self.res_block_21(out2)+self.shortcut_projection1(out2)\n",
        "\n",
        "        out4 = self.res_block_31(out3)+self.shortcut_projection2(out3)\n",
        "\n",
        "        out5 = torch.mean(out4, dim=-2)\n",
        "\n",
        "        out6 = self.fcn(out5)\n",
        "\n",
        "        out6 = out6 - torch.amax(out6, 1, keepdim=True)\n",
        "        out6 = self.LogSoftmax(out6)\n",
        "        return out6\n",
        "model = ConvFormerResNetFilterSmall().to(mps_device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34mFjcCkgYGh",
      "metadata": {
        "id": "34mFjcCkgYGh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FRWQeEGd61UK",
      "metadata": {
        "id": "FRWQeEGd61UK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "One ResNet block implemented as described in the ResNet paper (He, 2015).\n",
        "\"\"\"\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_in, num_out):\n",
        "        super().__init__()\n",
        "        self.num_in = num_in\n",
        "        self.num_out = num_out\n",
        "        self.stride = 1\n",
        "        self.conv2d_1 = nn.Conv2d(num_in, num_out, 3, stride = self.stride, padding = 1)\n",
        "        self.conv2d_2 = nn.Conv2d(num_in, num_out, 3, stride = self.stride, padding = 1)\n",
        "        self.batch_norm = nn.BatchNorm2d(num_out)\n",
        "        self.ReLU = nn.ReLU()\n",
        "    def forward(self, input):\n",
        "        out0 = self.ReLU(input)\n",
        "        out1 = self.conv2d_1(out0)\n",
        "        out2 = self.batch_norm(out1)\n",
        "        out3 = self.ReLU(out2)\n",
        "        out4 = self.conv2d_2(out3)\n",
        "        out5 = self.batch_norm(out4)\n",
        "        return out5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c4e99e-2fa1-446e-b4dd-d70539f9158f",
      "metadata": {
        "id": "75c4e99e-2fa1-446e-b4dd-d70539f9158f"
      },
      "outputs": [],
      "source": [
        "\"\"\"ResNet 20 implementation. Implemented using algorithms described in ResNet paper.\"\"\"\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16)\n",
        "        )\n",
        "        self.res_block_13 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16)\n",
        "        )\n",
        "        self.res_block_14 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16)\n",
        "        )\n",
        "        self.shortcut_projection1 = nn.Conv2d(16, 32, 1, stride = 2)\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding = 1, stride = 2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "        self.res_block_22 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "        self.res_block_23 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "        self.shortcut_projection2 = nn.Conv2d(32, 64, 1, stride = 2)\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding = 1, stride = 2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.res_block_32 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.res_block_33 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.pool = torch.nn.AvgPool2d(8, 1)\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        out1 = self.res_block_11(input)\n",
        "        ReLULayer = nn.ReLU()\n",
        "        out2 = ReLULayer(self.res_block_12(out1)+out1)\n",
        "        out3 = ReLULayer(self.res_block_13(out2)+out2)\n",
        "        out4 = ReLULayer(self.res_block_14(out3)+out3)\n",
        "        out5 = ReLULayer(self.res_block_21(out4)+self.shortcut_projection1(out4))\n",
        "        out6 = ReLULayer(self.res_block_22(out5)+out5)\n",
        "        out7 = ReLULayer(self.res_block_23(out6)+out6)\n",
        "        out8 = ReLULayer(self.res_block_31(out7)+self.shortcut_projection2(out7))\n",
        "        out9 = ReLULayer(self.res_block_32(out8)+out8)\n",
        "        out10 = ReLULayer(self.res_block_33(out9)+out9)\n",
        "        out10 = self.pool(out10)\n",
        "        out11 = self.fcn(out10.reshape(out10.shape[0],-1))\n",
        "        return out11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCNdulPD5eWE",
      "metadata": {
        "id": "nCNdulPD5eWE"
      },
      "outputs": [],
      "source": [
        "\"\"\"ResNet with changeable number of layers. Allows for the training of ResNet-20, 56, and 110 by changing the number n. N corresponds with the n in the ResNet paper used to adjust the size of CIFAR-10 networks.\"\"\"\n",
        "class ResNetChangeable(nn.Module):\n",
        "    def __init__(self, n = 9):\n",
        "        super().__init__()\n",
        "        self.patch_size = 4\n",
        "        self.seq_len = 1024/self.patch_size\n",
        "        self.inp_features = 3*self.patch_size\n",
        "        self.transform_inp = nn.Sequential(\n",
        "            Patchify(patch_size = int(math.sqrt(self.patch_size)))\n",
        "        )\n",
        "        self.res_block_11 = nn.Sequential(\n",
        "            nn.Conv2d(self.inp_features, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "        )\n",
        "        self.res_block_12 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, padding = 1),\n",
        "            nn.BatchNorm2d(16)\n",
        "        )\n",
        "        self.res_block_1rest = nn.Sequential(\n",
        "            *[ResBlock(16,16) for i in range(n-1)]\n",
        "        )\n",
        "        self.shortcut_projection1 = nn.Conv2d(16, 32, 1, stride = 2)\n",
        "        self.res_block_21 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, padding = 1, stride = 2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "        self.res_block_2rest = nn.Sequential(\n",
        "            *[ResBlock(32,32) for i in range(n-1)]\n",
        "        )\n",
        "        self.shortcut_projection2 = nn.Conv2d(32, 64, 1, stride = 2)\n",
        "        self.res_block_31 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding = 1, stride = 2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.res_block_3rest = nn.Sequential(\n",
        "            *[ResBlock(64,64) for i in range(n-1)]\n",
        "        )\n",
        "        self.pool = torch.nn.AvgPool2d(4, 1)\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        out1 = self.transform_inp(input).reshape(-1, int(math.sqrt(self.seq_len)), int(math.sqrt(self.seq_len)), self.inp_features).transpose(-1, 1).transpose(-1,-2)\n",
        "        out1 = self.res_block_11(out1)\n",
        "        out2 = self.res_block_12(out1)+out1\n",
        "        out3 = self.res_block_1rest(out2)+out2\n",
        "        out4 = self.res_block_21(out3)+self.shortcut_projection1(out3)\n",
        "        out5 = self.res_block_2rest(out4)+out4\n",
        "        out6 = self.res_block_31(out5)+self.shortcut_projection2(out5)\n",
        "        out7 = self.res_block_3rest(out6)+out6\n",
        "        out8 = self.pool(out7)\n",
        "        out9 = self.fcn(out8.reshape(out8.shape[0],-1))\n",
        "        return out9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031fb0b6-d0db-4e3a-9cae-1f66f271a29c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "031fb0b6-d0db-4e3a-9cae-1f66f271a29c",
        "outputId": "b9399fd4-3f9c-45e4-ab55-8b13a74080b9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Find parameter count for reduced size ConvFormers\"\"\"\n",
        "model = ConvFormerResNetSmall(img_h=32, img_w=32, patch_size=2).to(mps_device)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8666caa8-b216-4d69-b850-3f6474e6a305",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "8666caa8-b216-4d69-b850-3f6474e6a305",
        "outputId": "4d29828d-818d-4dab-9055-910d705763fe"
      },
      "outputs": [],
      "source": [
        "\"\"\"Find parameter count for ResNets\"\"\"\n",
        "model = ResNetChangeable(n=18).to(mps_device)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0574e2f-0870-43ce-b619-3f13521ccb9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0574e2f-0870-43ce-b619-3f13521ccb9b",
        "outputId": "4d103e2c-0027-4d0e-fb96-ebd818e2df2e"
      },
      "outputs": [],
      "source": [
        "\"\"\"Find parameter count for Vision Transformers\"\"\"\n",
        "model = TransformerResNetSmall(img_h=32, img_w=32).to(mps_device)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b3bca1-19e0-498d-97d0-d7edda68745f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "59b3bca1-19e0-498d-97d0-d7edda68745f",
        "outputId": "4046c1f4-60c8-4a97-bdb7-ccbbe202869e"
      },
      "outputs": [],
      "source": [
        "\"\"\"Training loop for the ResNets. Plots the validation accuracy along with the train and validation loss. Uses Learning Rate Plateau, SGD and weight decay\"\"\"\n",
        "import os\n",
        "from torcheval.metrics import MulticlassAccuracy\n",
        "\n",
        "test_num = 3\n",
        "hyperparam_dicts = [{\n",
        "    'weight_decay': [0.0001, 0.001, 0.01][i],\n",
        "} for i in range(test_num)]\n",
        "save_models = False\n",
        "start_model = 0\n",
        "if(start_model > 0):\n",
        "    model.load_state_dict(torch.load(f'ResNet/model{start_model}.pt'))\n",
        "epochs = 100-start_model\n",
        "if not os.path.exists(\"ResNet\"):\n",
        "    os.mkdir(\"ResNet\")\n",
        "final_accuracies = []\n",
        "# for hyperparam_dict in hyperparam_dicts:\n",
        "conv_model = ResNetChangeable(n=18).to(mps_device)\n",
        "optimizer = torch.optim.SGD(conv_model.parameters(), weight_decay=0.01, lr = 0.01, momentum = 0.9)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 10, mode='min')\n",
        "loss_fn = nn.CrossEntropyLoss().to(mps_device)\n",
        "accuracy_fn = MulticlassAccuracy()\n",
        "epochs = 100\n",
        "val_accuracy = []\n",
        "model_loss = []\n",
        "val_loss = []\n",
        "grad_mags = []\n",
        "clip_val = 3\n",
        "for j in range(epochs):\n",
        "    plt.show()\n",
        "    model_sub_loss = torch.zeros((len(CIFAR_Convloader),))\n",
        "    for i, batch in enumerate(CIFAR_Convloader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch[0].to(mps_device)\n",
        "        labels = batch[1].to(mps_device)\n",
        "        outputs = conv_model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        model_sub_loss[i] = loss.cpu().detach()\n",
        "        if(i%100==0):\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Convloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "        loss.backward()\n",
        "        if(i%100 == 0):\n",
        "            grad_mag = torch.norm(torch.stack([torch.norm(p.grad, 2.0) for p in conv_model.parameters() if p.grad is not None]), 2.0)\n",
        "            grad_mags.append(grad_mag)\n",
        "            print(grad_mag)\n",
        "        torch.nn.utils.clip_grad_norm_(conv_model.parameters(), clip_val)\n",
        "        optimizer.step()\n",
        "    model_loss.append(torch.mean(model_sub_loss,dim=-1))\n",
        "    print(\"Validation Stage:\")\n",
        "\n",
        "    dropout_modules = [module for module in model.modules() if isinstance(module,torch.nn.Dropout)]\n",
        "    [module.eval() for module in dropout_modules]\n",
        "    accuracy_fn.reset()\n",
        "    k = 0\n",
        "    val_sub_loss = torch.zeros((len(CIFAR_Valconvloader),))\n",
        "    for i, batch in enumerate(CIFAR_Valconvloader):\n",
        "        with torch.no_grad():\n",
        "            inputs = batch[0]\n",
        "            labels = batch[1]\n",
        "            inputs = inputs.to(mps_device)\n",
        "            labels = labels.to(mps_device)\n",
        "            outputs = conv_model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_sub_loss[k] = loss.cpu().detach()\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            if(k%10==0):\n",
        "                print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Valconvloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "            k+=1\n",
        "    lr_scheduler.step(torch.mean(val_sub_loss))\n",
        "    val_loss.append(torch.mean(val_sub_loss))\n",
        "    print(f\"Final Accuracy: {accuracy_fn.compute()}\")\n",
        "    val_accuracy.append(accuracy_fn.compute())\n",
        "    accuracy_fn.reset()\n",
        "    plt.plot(model_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.show()\n",
        "    plt.plot(val_accuracy)\n",
        "    plt.show()\n",
        "    plt.imshow(inputs[0].permute(1,2,0).cpu().detach())\n",
        "    plt.show()\n",
        "    [module.train() for module in dropout_modules]\n",
        "    torch.save(model.state_dict(),f\"ResNet/model{j}.pt\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fX70cbSGQ2AW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "fX70cbSGQ2AW",
        "outputId": "a54cd17f-c1f3-4a1f-eeca-0ab128ee5e66"
      },
      "outputs": [],
      "source": [
        "\"\"\"Save training state of ConvFormers\"\"\"\n",
        "torch.save(model.state_dict(),f\"/ConvFormerResNet1/model{j}_final.pt\")\n",
        "torch.save(optimizer.state_dict(),f\"/ConvFormerResNet1/model{j}_optim.pt\")\n",
        "torch.save(lr_scheduler.state_dict(),f\"/ConvFormerResNet1/model{j}_scheduler.pt\")\n",
        "\"\"\"Save ResNet metrics\"\"\"\n",
        "with open(\"ResNet/grad_mags.txt\", 'w+') as writer:\n",
        "    for grad_mag in grad_mags:\n",
        "        writer.write(f\"{grad_mag},\")\n",
        "with open(\"ResNet/val_loss.txt\", 'w+') as writer:\n",
        "    for loss in val_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"ResNet/train_loss.txt\", 'w+') as writer:\n",
        "    for loss in model_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"ResNet/val_accuracy.txt\", 'w+') as writer:\n",
        "    for accuracy in val_accuracy:\n",
        "        writer.write(f\"{accuracy},\")\n",
        "plt.plot(model_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.savefig(f\"ResNet/TrainVsValidationLoss.png\")\n",
        "plt.show()\n",
        "plt.plot(val_accuracy)\n",
        "plt.savefig(f\"ResNet/ValidationAccuracy.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RWUseTgnjb3x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWUseTgnjb3x",
        "outputId": "c7cbeb30-b588-45b7-8a94-e7ff0f0e196d"
      },
      "outputs": [],
      "source": [
        "\"\"\"Zip ResNet models\"\"\"\n",
        "!zip -r /ResNetModel20PatchedPt.zip /ResNet/*_*.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NsmsF960YkKF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsmsF960YkKF",
        "outputId": "14aa153f-9d37-4fdd-9342-22a378350e8d"
      },
      "outputs": [],
      "source": [
        "\"\"\"Zip ResNet metrics\"\"\"\n",
        "!zip -r /ResNetModel20PatchedTxt.zip /ResNet/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32cf285-a4c8-45bc-ac5d-97e463156212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c32cf285-a4c8-45bc-ac5d-97e463156212",
        "outputId": "6360212c-6b87-49df-f565-ad21150ee25f"
      },
      "outputs": [],
      "source": [
        "\"\"\"Training loop for ConvFormers. Plots the validation accuracy along with the train and validation loss. Uses Learning Rate Plateau, SGD and weight decay. Also plots one learnable mask to show the effect of GradientBiasing\"\"\"\n",
        "import os\n",
        "from torcheval.metrics import MulticlassAccuracy\n",
        "model = ConvFormerResNetSmall(img_h = 32, img_w = 32, patch_size=2).to(mps_device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), weight_decay=0.003, lr = 0.01, momentum = 0.9)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 10)\n",
        "start_model = 0\n",
        "if not os.path.exists(\"ConvFormerResNet\"):\n",
        "    os.mkdir(\"ConvFormerResNet\")\n",
        "if(start_model > 0):\n",
        "    model.load_state_dict(torch.load(f'ConvFormerResNet/model{start_model-1}.pt'))\n",
        "    optimizer.load_state_dict(torch.load(f'ConvFormerResNet/model{start_model-1}_optim.pt'))\n",
        "    lr_scheduler.load_state_dict(torch.load(f'ConvFormerResNet/model{start_model-1}_scheduler.pt'))\n",
        "loss_fn = nn.NLLLoss().to(mps_device)\n",
        "accuracy_fn = MulticlassAccuracy()\n",
        "epochs = 100-start_model\n",
        "val_accuracy = []\n",
        "model_loss = []\n",
        "val_loss = []\n",
        "grad_mags = []\n",
        "clip_val = 3\n",
        "start_batch = 0\n",
        "prev_matrix = torch.gather(torch.matmul(model.res_block_11[0].mask1.transpose(-1,-2),model.res_block_11[0].mask2).cpu().detach()[0], -1, model.res_block_11[0].roll_back.cpu().detach())\n",
        "plt.matshow(prev_matrix)\n",
        "plt.show()\n",
        "for j in range(epochs):\n",
        "    model_sub_loss = torch.zeros((len(CIFAR_Convloader),))\n",
        "    model_sub_loss.requires_grad = False\n",
        "    for i, batch in enumerate(CIFAR_Convloader):\n",
        "        if j == 0 and i < start_batch:\n",
        "            continue\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        inputs = batch[0]\n",
        "        labels = batch[1]\n",
        "        inputs = inputs.to(mps_device)\n",
        "        labels = labels.to(mps_device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        model_sub_loss[i] = loss.cpu().detach()\n",
        "        if(i%100==0):\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Convloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "        loss.backward()\n",
        "        if(i%100 == 0):\n",
        "            grad_mag = torch.norm(torch.stack([torch.norm(p.grad, 2.0) for p in model.parameters() if p.grad is not None]), 2.0)\n",
        "            grad_mags.append(grad_mag)\n",
        "            print(grad_mag)\n",
        "            torch.cuda.empty_cache()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
        "        optimizer.step()\n",
        "    model_loss.append(torch.mean(model_sub_loss,dim=-1))\n",
        "    print(\"Validation Stage:\")\n",
        "    dropout_modules = [module for module in model.modules() if isinstance(module,torch.nn.Dropout)]\n",
        "    [module.eval() for module in dropout_modules]\n",
        "    accuracy_fn.reset()\n",
        "    k = 0\n",
        "    val_sub_loss = torch.zeros((len(CIFAR_Valconvloader),))\n",
        "    val_sub_loss.requires_grad = False\n",
        "    for i, batch in enumerate(CIFAR_Valconvloader):\n",
        "        with torch.no_grad():\n",
        "            inputs = batch[0]\n",
        "            labels = batch[1]\n",
        "            inputs = inputs.to(mps_device)\n",
        "            labels = labels.to(mps_device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_sub_loss[k] = loss.cpu().detach()\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            if(k%10==0):\n",
        "                print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Valconvloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "            k+=1\n",
        "    print(f\"Final Accuracy: {accuracy_fn.compute()}\")\n",
        "    lr_scheduler.step(torch.mean(val_sub_loss))\n",
        "    val_loss.append(torch.mean(val_sub_loss))\n",
        "    val_accuracy.append(accuracy_fn.compute())\n",
        "    accuracy_fn.reset()\n",
        "    plt.plot(model_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.show()\n",
        "    plt.plot(val_accuracy)\n",
        "    plt.show()\n",
        "    plt.imshow(inputs[0].permute(1,2,0).cpu().detach())\n",
        "    plt.show()\n",
        "    plt.matshow(torch.gather(torch.matmul(model.res_block_11[0].mask1.transpose(-1,-2),model.res_block_11[0].mask2).cpu().detach()[0], -1, model.res_block_11[0].roll_back.cpu().detach()))\n",
        "    plt.show()\n",
        "    print(f\"lr: {lr_scheduler._last_lr }\")\n",
        "    print(f\"mask_shape = {model.res_block_11[0].mask1.shape}\")\n",
        "    [module.train() for module in dropout_modules]\n",
        "    torch.save(model.state_dict(),f\"ConvFormerResNet/model{j+start_model}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924385f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Save training state of ConvFormers\"\"\"\n",
        "torch.save(model.state_dict(),f\"ConvFormerResNet/model{j+start_model-1}_final.pt\")\n",
        "torch.save(optimizer.state_dict(),f\"ConvFormerResNet/model{j+start_model-1}_optim.pt\")\n",
        "torch.save(lr_scheduler.state_dict(),f\"ConvFormerResNet/model{j+start_model-1}_scheduler.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae404662",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Save ConvFormer metrics\"\"\"\n",
        "if not os.path.exists(\"ConvFormerResNet\"):\n",
        "    os.mkdir(\"ConvFormerResNet\")\n",
        "with open(\"ConvFormerResNet/grad_mags.txt\", 'w+') as writer:\n",
        "    for grad_mag in grad_mags:\n",
        "        writer.write(f\"{grad_mag},\")\n",
        "with open(\"ConvFormerResNet/val_loss.txt\", 'w+') as writer:\n",
        "    for loss in val_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"ConvFormerResNet/train_loss.txt\", 'w+') as writer:\n",
        "    for loss in model_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"ConvFormerResNet/val_accuracy.txt\", 'w+') as writer:\n",
        "    for accuracy in val_accuracy:\n",
        "        writer.write(f\"{accuracy},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be8273e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Training loop for ConvFormers. Plots the validation accuracy along with the train and validation loss. Uses Learning Rate Plateau, SGD and weight decay. Also plots one learnable mask to show the effect of GradientBiasing\"\"\"\n",
        "import os\n",
        "from torcheval.metrics import MulticlassAccuracy\n",
        "model = TransformerResNetSmall(img_h = 32, img_w = 32, patch_size=2).to(mps_device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), weight_decay=0.001, lr = 0.1, momentum = 0.9)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 10)\n",
        "start_model = 0\n",
        "if not os.path.exists(\"TransformerResNet\"):\n",
        "    os.mkdir(\"TransformerResNet\")\n",
        "if(start_model > 0):\n",
        "    model.load_state_dict(torch.load(f'TransformerResNet/model{start_model-1}.pt'))\n",
        "    optimizer.load_state_dict(torch.load(f'TransformerResNet/model{start_model-1}_optim.pt'))\n",
        "    lr_scheduler.load_state_dict(torch.load(f'TransformerResNet/model{start_model-1}_scheduler.pt'))\n",
        "loss_fn = nn.NLLLoss().to(mps_device)\n",
        "accuracy_fn = MulticlassAccuracy()\n",
        "epochs = 100-start_model\n",
        "val_accuracy = []\n",
        "model_loss = []\n",
        "val_loss = []\n",
        "grad_mags = []\n",
        "clip_val = 3\n",
        "start_batch = 0\n",
        "for j in range(epochs):\n",
        "    model_sub_loss = torch.zeros((len(CIFAR_Convloader),))\n",
        "    model_sub_loss.requires_grad = False\n",
        "    for i, batch in enumerate(CIFAR_Convloader):\n",
        "        if j == 0 and i < start_batch:\n",
        "            continue\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        inputs = batch[0]\n",
        "        labels = batch[1]\n",
        "        inputs = inputs.to(mps_device)\n",
        "        labels = labels.to(mps_device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        model_sub_loss[i] = loss.cpu().detach()\n",
        "        if(i%100==0):\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Convloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "        loss.backward()\n",
        "        if(i%100 == 0):\n",
        "            grad_mag = torch.norm(torch.stack([torch.norm(p.grad, 2.0) for p in model.parameters() if p.grad is not None]), 2.0)\n",
        "            grad_mags.append(grad_mag)\n",
        "            print(grad_mag)\n",
        "            torch.cuda.empty_cache()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
        "        optimizer.step()\n",
        "    model_loss.append(torch.mean(model_sub_loss,dim=-1))\n",
        "    print(\"Validation Stage:\")\n",
        "    dropout_modules = [module for module in model.modules() if isinstance(module,torch.nn.Dropout)]\n",
        "    [module.eval() for module in dropout_modules]\n",
        "    accuracy_fn.reset()\n",
        "    k = 0\n",
        "    val_sub_loss = torch.zeros((len(CIFAR_Valconvloader),))\n",
        "    val_sub_loss.requires_grad = False\n",
        "    for i, batch in enumerate(CIFAR_Valconvloader):\n",
        "        with torch.no_grad():\n",
        "            inputs = batch[0]\n",
        "            labels = batch[1]\n",
        "            inputs = inputs.to(mps_device)\n",
        "            labels = labels.to(mps_device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_sub_loss[k] = loss.cpu().detach()\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            if(k%10==0):\n",
        "                print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Valconvloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "            k+=1\n",
        "    print(f\"Final Accuracy: {accuracy_fn.compute()}\")\n",
        "    lr_scheduler.step(torch.mean(val_sub_loss))\n",
        "    val_loss.append(torch.mean(val_sub_loss))\n",
        "    val_accuracy.append(accuracy_fn.compute())\n",
        "    accuracy_fn.reset()\n",
        "    plt.plot(model_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.show()\n",
        "    plt.plot(val_accuracy)\n",
        "    plt.show()\n",
        "    plt.imshow(inputs[0].permute(1,2,0).cpu().detach())\n",
        "    plt.show()\n",
        "    print(f\"lr: {lr_scheduler._last_lr }\")\n",
        "    [module.train() for module in dropout_modules]\n",
        "    torch.save(model.state_dict(),f\"TransformerResNet/model{j+start_model}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e0ee0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Save training state of ConvFormers\"\"\"\n",
        "torch.save(model.state_dict(),f\"TransformerResNet/model{j+start_model-1}_final.pt\")\n",
        "torch.save(optimizer.state_dict(),f\"TransformerResNet/model{j+start_model-1}_optim.pt\")\n",
        "torch.save(lr_scheduler.state_dict(),f\"TransformerResNet/model{j+start_model-1}_scheduler.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b58bf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Save ConvFormer metrics\"\"\"\n",
        "if not os.path.exists(\"TransformerResNet\"):\n",
        "    os.mkdir(\"TransformerResNet\")\n",
        "with open(\"TransformerResNet/grad_mags.txt\", 'w+') as writer:\n",
        "    for grad_mag in grad_mags:\n",
        "        writer.write(f\"{grad_mag},\")\n",
        "with open(\"TransformerResNet/val_loss.txt\", 'w+') as writer:\n",
        "    for loss in val_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"TransformerResNet/train_loss.txt\", 'w+') as writer:\n",
        "    for loss in model_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"TransformerResNet/val_accuracy.txt\", 'w+') as writer:\n",
        "    for accuracy in val_accuracy:\n",
        "        writer.write(f\"{accuracy},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb35e0c0-2b86-467e-95b8-cb8309a42b47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cb35e0c0-2b86-467e-95b8-cb8309a42b47",
        "outputId": "8a91d8ad-ab96-418e-aa45-5c4e88b4739f"
      },
      "outputs": [],
      "source": [
        "\"\"\"Training loop for Transformers and Vision Transformers. Plots the validation accuracy along with the train and validation loss. Uses Learning Rate Plateau, SGD and weight decay.\"\"\"\n",
        "import os\n",
        "from torcheval.metrics import MulticlassAccuracy\n",
        "model = VisionTransformer(img_h = 32, img_w = 32).to(mps_device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), weight_decay=0.01, lr = 0.01, momentum = 0.9)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 10)\n",
        "start_model = 0\n",
        "if(start_model > 0):\n",
        "    model.load_state_dict(torch.load(f'VisionTransformer/model{start_model-1}.pt'))\n",
        "    optimizer.load_state_dict(torch.load(f'VisionTransformer/model{start_model-1}_optim.pt'))\n",
        "    lr_scheduler.load_state_dict(torch.load(f'VisionTransformer/model{start_model-1}_scheduler.pt'))\n",
        "loss_fn = nn.CrossEntropyLoss().to(mps_device)\n",
        "accuracy_fn = MulticlassAccuracy()\n",
        "epochs = 100-start_model\n",
        "val_accuracy = []\n",
        "model_loss = []\n",
        "val_loss = []\n",
        "grad_mags = []\n",
        "if not os.path.exists(\"VisionTransformer\"):\n",
        "    os.mkdir(\"VisionTransformer\")\n",
        "for j in range(epochs):\n",
        "    model_sub_loss = torch.zeros((len(CIFAR_Convloader),))\n",
        "    model_sub_loss.requires_grad = False\n",
        "    for i, batch in enumerate(CIFAR_Convloader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch[0]\n",
        "        labels = batch[1]\n",
        "        inputs = inputs.to(mps_device)\n",
        "        labels = labels.to(mps_device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        model_sub_loss[i] = loss.cpu().detach()\n",
        "        if(i%100==0):\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            print(labels)\n",
        "            print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Convloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "        loss.backward()\n",
        "        if(i%100 == 0):\n",
        "            grad_mag = torch.norm(torch.stack([torch.norm(p.grad, 2.0) for p in model.parameters() if p.grad is not None]), 2.0)\n",
        "            grad_mags.append(grad_mag)\n",
        "            print(grad_mag)\n",
        "            torch.cuda.empty_cache()\n",
        "        optimizer.step()\n",
        "    model_loss.append(torch.mean(model_sub_loss,dim=-1))\n",
        "    print(\"Validation Stage:\")\n",
        "    dropout_modules = [module for module in model.modules() if isinstance(module,torch.nn.Dropout)]\n",
        "    [module.eval() for module in dropout_modules]\n",
        "    accuracy_fn.reset()\n",
        "    k = 0\n",
        "    val_sub_loss = torch.zeros((len(CIFAR_Valconvloader),))\n",
        "    val_sub_loss.requires_grad = False\n",
        "    for i, batch in enumerate(CIFAR_Valconvloader):\n",
        "        with torch.no_grad():\n",
        "            inputs = batch[0]\n",
        "            labels = batch[1]\n",
        "            inputs = inputs.to(mps_device)\n",
        "            labels = labels.to(mps_device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_sub_loss[k] = loss.cpu().detach()\n",
        "            accuracy_fn.update(outputs, labels)\n",
        "            if(k%10==0):\n",
        "                print(labels)\n",
        "                print(f\"Loss: {loss}, Batch Num: {i}/{len(CIFAR_Valconvloader)}, Accuracy:{accuracy_fn.compute()}, Epoch: {j+start_model}\")\n",
        "            k+=1\n",
        "    print(f\"Final Accuracy: {accuracy_fn.compute()}\")\n",
        "    lr_scheduler.step(torch.mean(val_sub_loss))\n",
        "    val_loss.append(torch.mean(val_sub_loss))\n",
        "    val_accuracy.append(accuracy_fn.compute())\n",
        "    accuracy_fn.reset()\n",
        "    plt.plot(model_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.show()\n",
        "    plt.plot(val_accuracy)\n",
        "    plt.show()\n",
        "    plt.imshow(inputs[0].permute(1,2,0).cpu().detach())\n",
        "    plt.show()\n",
        "    [module.train() for module in dropout_modules]\n",
        "    torch.save(model.state_dict(),f\"VisionTransformer/model{j+start_model}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aTL7EZIW7qzv",
      "metadata": {
        "id": "aTL7EZIW7qzv"
      },
      "outputs": [],
      "source": [
        "\"\"\"Save VisionTransformer training state\"\"\"\n",
        "torch.save(model.state_dict(),f\"VisionTransformer/model{j+start_model-1}_final.pt\")\n",
        "torch.save(optimizer.state_dict(),f\"VisionTransformer/model{j+start_model-1}_optim.pt\")\n",
        "torch.save(lr_scheduler.state_dict(),f\"VisionTransformer/model{j+start_model-1}_scheduler.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4522e491-2721-4df3-905a-8922e24b25f4",
      "metadata": {
        "id": "4522e491-2721-4df3-905a-8922e24b25f4"
      },
      "outputs": [],
      "source": [
        "\"\"\"Save VisionTransformer metrics\"\"\"\n",
        "with open(\"VisionTransformer/grad_mags.txt\", 'w+') as writer:\n",
        "    for grad_mag in grad_mags:\n",
        "        writer.write(f\"{grad_mag},\")\n",
        "with open(\"VisionTransformer/val_loss.txt\", 'w+') as writer:\n",
        "    for loss in val_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"VisionTransformer/train_loss.txt\", 'w+') as writer:\n",
        "    for loss in model_loss:\n",
        "        writer.write(f\"{loss},\")\n",
        "with open(\"VisionTransformer/accuracy.txt\", 'w+') as writer:\n",
        "    for accuracy in val_accuracy:\n",
        "        writer.write(f\"{val_accuracy},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "muvNCGpP8wEC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muvNCGpP8wEC",
        "outputId": "4bc19742-9495-4f31-ee43-e981f7b06532"
      },
      "outputs": [],
      "source": [
        "\"\"\"Zip Vision Transformer metrics\"\"\"\n",
        "!zip -r /VisionTransformerTxt.zip /VisionTransformer/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QOiJu_hHE6qU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOiJu_hHE6qU",
        "outputId": "1d459b91-240e-4562-93d3-6d98373f6748"
      },
      "outputs": [],
      "source": [
        "\"\"\"Zip Vision Transformer model\"\"\"\n",
        "!zip -r /VisionTransformerModel.zip /VisionTransformer/*_*.pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
